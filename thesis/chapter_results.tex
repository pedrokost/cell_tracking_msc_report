\chapter{Experimental results \status{in progress}}
\label{chap:results}

In this chapter we quantitatively and qualitatively analyse the performance of the automatic cell detector and tracker. Although some evaluation of the performance of the detection method is performed by the original authors in \cite{arteta12} it is useful to see how the method performs on the studied datasets in order to understand how much of the tracking accuracy is lost due to cells missed by the detection module. First, in \cref{sec:results_detector} we evaluate the performance and computation time of the cell detector and in \cref{sec:results_tracker} those of the cell tracker. Finally, in \cref{sec:results_limitations}, we explore the limitations of the methods and in \cref{sec:results_summary} summarize the results.

\todo[inline]{See the cell population tracking and linear construction with spationtemporal ocntet by Kang et al for a good results section}

\section{Cell detector \status{in progress}}
	\label{sec:results_detector}
	
	In this section we evaluated the performance of the automatic cell detection module. First, we introduce the performance metrics used to evaluate the accuracy of the cell detector. Then we present detection accuracy results. To evaluate the accuracy and generalizability of the detection module we perform two sets of experiments. First, we train the cell detector on a number of frames from each individual dataset, and measure the accuracy on the same dataset. Second, we train the detector on combinations of datasets in order to judge the performance degradation due to the learning on the more types of cells. Because of the varying size of the cells in the datasets, and the varying brightness of the cells, we expect that such a trained detector will perform poorer than when trained and tested on individual datasets, sometimes mistakenly detecting small artefacts in the background as cells. Finally, we compute the average detection time per frame for each dataset.
	
	The aim of this research was to develop an automatic cell detection and tracking pipeline that would require as little manual work as possible. This implies that a balance between accuracy and amount of manual work had to be established. There is also an direct relationship between accuracy and computation time. In order to reduce the amount of manual work we aimed to configure the cell detection module such that it would perform well on all the tested datasets without any manual adjustments of parameters. The consequences of this decision are twofold:
	
	\begin{enumerate}
		\item The features computed on the candidate cell regions are the same for all datasets and have been presented in \cref{sec:detector_feature}. Although some datasets could be analysed faster or more accurately with a different subset of features, using the same features for all dataset eliminates the complicated feature selection process for the user and makes the system generalizable to a large number of different cell types.
		
		\item The parameters of the MSER detector should be adequately set to perform well on all datasets. This means that the MSER detector should be able to detect cells of varying size and contrast in the different datasets. The consequence of this limitation for datasets with large cells and some background noise is that a potentially much larger number of candidate regions will be detected than necessary. Since each candidate region has to be evaluated this results in an increased computation time.
	\end{enumerate}
	
	We were able to identify features that compute in an acceptable time for all these datasets (see \cref{sec:detector_feature}). However, it should be noted that in the case of testing the detector on very large datasets with thousands of frames, some adjustments of the parameters could result in a significant reduction in computation time and increased accuracy.
	
	\subsection{Performance metrics \statusnew}
	
	We measure the performance of the cell detector in terms of precision and recall. The metrics are defined in terms of:
	
	\begin{description}
		\item[True Positive instances] (TP) are candidate cell regions that are manually annotated as cells and the detector successfully classified as cells.
		\item[False Positive instances] (FP) are candidate cell regions that are not manually annotated as cells, but the detector incorrectly classified them as cells.
		\item[False Negative instances] (FN) are candidate cell regions that are manually annotated as cells, but the detector incorrectly classified.
	\end{description}
	
	We then define precision as:
	
	\[
		\text{PRE} = \frac{\text{TP}}{\text{TP}+\text{FP}}\text{,}
	\]
	
	\noindent and recall (also known as sensitivity) as:
	
	\[
		\text{REC} = \frac{\text{TP}}{\text{TP} + \text{FN}}\ \text{.}
	\]
	
	\subsection{Detection accuracy \statusnew}
			
		As mentioned previously, we performed two different experiments to measure the performance of the cell detection module. The first experiment consisted of training and testing of the algorithm on the same dataset. The training set was 70\% and the testing set 30\% of the entire dataset. The training and testing datasets were created from images in randomized order from the entire dataset. This allowed us to measure, for each dataset, the maximum precision and recall values we could expect from the algorithm. In the second experiment the training was performed on combined datasets. The goal of this experiment was to observe how well the algorithm is able to generalize, and still return acceptable results.
		
		\subsubsection{Training and testing on single dataset}
		
		The training was performed on 70\% of the manually dot-annotated images from each dataset, and tested on the remaining 30\%. \Cref{tab:results_detector_individual} displays the computed precision and recall values, together with the total number of cells that were annotated in each dataset.
		
		\todo{Repeat the measurements with the best feature selection}
		
		\begin{table}[h]
			\centering
			\begin{tabular}{rrr}
				Dataset & Precision & Recall \\
				\hline
				      A &      25.0 &   22.5 \\
				      B &      90.1 &   89.1 \\
				      C &      76.5 &   84.2 \\
				      D &      86.1 &   85.3 \\
				      E &      93.4 &   78.6
			\end{tabular} 
			\caption{Precision and recall values for the cell detector trained on each dataset individually.}
			\label{tab:results_detector_individual}
		\end{table}
		
		The detector tested on datasets B to E achieved precision and recall values above 75. A manual comparison of the annotation confirms that the results are good, with few bad detections. Most of the differences in detection were caused by minor inconsistencies in annotations. The importance of consistent annotations was stressed in \cref{sec:data_manual_annotation}.
		
		Dataset A is an outlier with extremely low precision and recall values. This is likely caused by the specific characteristics of this image sequence, which we described in \cref{subsec:datasetA}. Briefly, between frame 17 and 18 there is an abrupt change of image clarity. The background from the threshold frame onwards have a texture that is very similar to that of the cells in the first 17 frames. This means that the negative candidate regions from frame 18 and onward clash with the positive candidate regions from the previous frames. The result is that the detector is unable to learn to discriminate cells from background.
		
		These results show us the detector can correctly detect most of the (annotated) cells. Further manual reviews of the annotations would likely improve the performance, but this is not done here as it should have been tested on a separate validation dataset. Additionally, it is unlikely that future users of this detection method will always have perfect annotations available. In the next experiment, we will measured the performance of the detector when trained on a composite dataset. This will tell us whether it would be possible to learn a single, general detector and use it to test on a new, possibly unforeseen datasets.

		\Cref{fig:results_detector_sequences_1,fig:results_detector_sequences_2,fig:results_detector_sequences_3,fig:results_detector_sequences_4,fig:results_detector_sequences_5} in appendix \ref{app:appendix_detectionresults} display a temporal view of the detected results in each datasets. The vertical axis represents the consecutive frame number of the image sequence. The figures show that ``cell tracks'' are discernible, even if the number of outliers is significant. The detectors used to detect the cells on the entire dataset were trained on all the annotated frames. \todo{Repeat the training+testing using all frames for training}

		\subsubsection{Training on composite dataset}
		
		In this second experiment we were interested in measuring how well the algorithm is able to generalize when it is trained on a larger, combined dataset. For this purpose, several of the datasets were grouped, and the detector was trained to recognize cells of all types. The detector was trained on a random 70\% of all annotated images in the combined dataset. It was then tested on a random 30\% of annotated frames from each individual dataset separately. This means that sometimes the same frames could be used for training and testing. However, we also run combinations of datasets where one dataset is left out of training at each time. Testing on that dataset should then reveal if the algorithm generalizes well.
		
		\Cref{tab:results_detector_combined} summarizes the precision and recall values for all tested combined datasets. The column denoted $\gamma$ contains testing performance as tested on 30\% of the combined dataset. The values shown in bold correspond to the performance of testing on the dataset that was left out from the combined training dataset.
		
		\todo{Repeat the measurements with the best feature selection}
		
		\todo{Try different dataset combinations based on the observations... similar datasets}

		
					%	      A &      25.0 &   22.5 \\
					%	      B &      90.1 &   89.1 \\
					%	      C &      76.5 &   84.2 \\
					%	      D &      86.1 &   85.3\\
					%	      E &      93.4 &   78.6\\
		
					% > 1 point better : blue, > 1 point worse: red, else black
		\begin{table}[h]
			\centering
			\begin{tabular}{p{3cm}|*{6}{c}|*{6}{c}}
				\multicolumn{1}{c}{Combined dataset} & \multicolumn{6}{|c|}{Precision}                                             &                         \multicolumn{6}{c}{Recall}                          \\
				                                     & $\gamma$ &     A     &     B      &     C      &     D      &       E       & $\gamma$ &     A     &       B       &     C      &     D      &     E      \\
			\hline
				ABCDE                                &   74.5   & \up{49.2} & \dn{76.0}  & \up{85.2}  & \dn{69.2}  &   \up{94.5}   &   79.9   & \up{58.3} &   \up{90.8}   & \dn{57.1}  & \up{98.9}  & \up{79.7}  \\
			\hline
				ABCD\_                               &   69.7   & \up{49.2} & \dn{77.0}  & \up{85.8}  & \dn{69.9}  & \textbf{93.4} &   80.0   & \up{58.3} &     90.3      & \dn{58.7}  & \up{98.4}  & \upd{80.7} \\
				ABC\_E                               &   72.8   & \up{45.9} & \dn{75.0}  & \up{82.6}  & \dnd{41.1} &   \dn{87.2}   &   83.2   & \up{65.0} &   \up{96.4}   & \dn{73.6}  & \upd{99.4} & \up{89.8}  \\
				AB\_DE                               &   75.5   & \up{47.5} & \dn{75.8}  & \upd{85.7} & \dn{73.1}  &   \up{96.5}   &   73.1   & \up{50.8} &     88.9      & \dnd{46.1} & \up{97.1}  & \dn{68.3}  \\
				A\_CDE                               &   75.9   & \up{41.3} & \dnd{63.1} & \up{85.5}  & \dn{71.2}  &   \up{94.6}   &   70.4   & \up{58.3} & \textbf{89.2} & \dn{57.0}  & \up{99.0}  & \up{80.2}  \\
				\_BCDE                               &   86.3   &   49.2    &    76.4    &    85.9    &    71.8    &     94.4      &   76.4   &   58.3    &     89.5      &    56.0    &    97.4    &    75.7    \\
			\hline
			\end{tabular} 
			\caption{Precision and recall values for the cell detector trained on combined datasets. Values typed in bold indicate the testing datasets that were not included in the combined training dataset. Red/blue colour indicates a decrease/increase in performance compared to training and testing on each individual dataset by at least 1 precision/recall point.}
			\label{tab:results_detector_combined}
		\end{table}
				
		\todo{Add discussion: generally how much precision/recall we loose, which dataset seems to suffer less/more, why do they suffer less/more (have fewer cells to be trained on, like like noise in another dataset)}
		
		Answer: which improve, which worsen. By how much? Could we still use the results for tracking?
		Does training on similar datasets than testing help? In general higher recall: good for tracker, since lower precision can be fixed easier since it is easier to identify false positive tracklets than correctly link tracklets that have many missing detections.
		
		Which dataset worsens prec and recall when combined?  none
		Which dataset improves prec and recall when combined?  A, E mostly
		Which dataset improve precision when combined? A, C, E mostly
		Which datasets improve recall when combined?  A, B mostly, D, E mostly
		Eliminating which datasets improves everything?  no clear winner
		Elimintating which dataset worsens everything?  no clear winner
		
		Looking just at the non training dataset:
		Which dataset worsens prec and recall when combined?  
		Which dataset improves prec and recall when combined?
		Which dataset worsens all others when combined?  B mostly
		Which dataset improves all others when combined?    E mostly
		Which dataset improve precision when combined? C
		Which datasets improve recall when combined?   D, E
		
		
		
		Wins
			
		Looses
	\subsection{Computations time \statusnew}
	
		In order to positionally track cells in image sequence the computation time of extracting the cell position from each frame is just as important as the accurate identification of cells. \Cref{tab:results_detector_speed} displays the average detection times per frame for each dataset. The detection time is dependent on the number of candidate cell regions extracted and their dimensions.
		
		\todo{Explain hardware and software, say 1 core... can use more}
		
		\todo{It might be useful to show the comparison with the old detector if I have the time}
		
		\todo[inline]{Measure the speed of detection in images of different sizes, and different number of cells}
		
		\todo[inline]{Explain what the candidate-annoation ration is, and what it tells. Also the ocmputations timer per candidate vary from 0.0086-0.0226}
		
		\begin{table}[h]
			\centering
			\begin{tabular}{rrrr}
				Dataset & Time per frame  $\left[ s \right]$ & Time per annotated cell  $\left[ s \right]$ & Candidate-Annotation ratio \\
			\hline
				      A &                             0.9224 &                                      0.5854 &                       26:1 \\
				      B &                             1.5899 &                                      0.2015 &                       16:1 \\
				      C &                             1.1214 &                                      0.0586 &                        7:1 \\
				      D &                             1.5215 &                                      0.1454 &                       18:1 \\
				      E &                             0.6016 &                                      0.0853 &                        9:1
			\end{tabular} 
			\caption{Average computation times per frame and annotated cell using a single worker.}
			\label{tab:results_detector_speed}
		\end{table}
		
		\todo{It seems computation time very dependend on size of candidates cells: so maybe can achieve faster results by first scaling down the images}
		\todo{Discuss which is faster, try to explain why-- less noise -- less candidate cells... }
	
\section{Cell tracker \statusnew}
	\label{sec:results_tracker}
	\todo[inline]{Define the different measures of accuracy}
	\subsection{Performance metrics \statusnew}
	
	TODO: test detector on full dataset, but train it on ALL (notjust70\%) of the annotation
	TODO: train tracker on 70\% of the trajectories, test on rest
	
	great Metrics: Research Article, Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics
	
	- Explain how the testing data was generation. from annotation to mapped deteciton to genreated tracklets

	\subsection{Tracking accuracy \statusnew}
		- trained on single dataset
		
		- for each dataset explain how the parameters were setup
					
		- trained on combined dataset
	\subsection{Computation time \statusnew}
	\todo[inline]{Meause the speed of generating tracks, as a measure of per 1, 100, 1000 frames, depending on the number of tracks}
\section{Limitations and areas of improvement \statusnew}
	\label{sec:results_limitations}
	Answer: what, why, how to improve in future
			
	- display examples where the tracker did not perform well, and anlyse why. Suggest possible improvement.
	- detection training: only first few frames of datasets, not random -- expect to detect later frames worse
	- testing on only long datasets: no data on short datasets. diffucult to train (what to link?), difficult to annotate
	- speed of detector. Reduce number of hypothesis		

\section{Summary \statusnew}
	\label{sec:results_summary}
	Brief review of accuracy... whether it is comparable to other methods in literuature review
	Whether is could be improved in the future... how much