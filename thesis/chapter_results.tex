\chapter{Experimental results \status{in progress}}
\label{chap:results}

In this chapter we quantitatively and qualitatively analyse the performance of the automatic cell detector and tracker. Although some evaluation of the performance of the detection method is performed by the original authors in \cite{arteta12} it is useful to see how the method performs on the studied datasets in order to understand how much of the tracking accuracy is lost due to cells missed by the detection module. First, in \cref{sec:results_detector} we evaluate the performance and computation time of the cell detector and in \cref{sec:results_tracker} those of the cell tracker. Finally, in \cref{sec:results_limitations}, we explore the limitations of the methods and in \cref{sec:results_summary} summarize the results.

\todo[inline]{See the cell population tracking and linear construction with spationtemporal ocntet by Kang et al for a good results section}

\section{Cell detector \statusfirstdraft}
	\label{sec:results_detector}
	
	In this section we evaluate the performance of the automatic cell detection module. First, we introduce the performance metrics used to evaluate the accuracy of the cell detector. Then we present detection accuracy results. To evaluate the accuracy and generalizability of the detection module we perform two sets of experiments. First, we train the cell detector on a number of frames from each individual dataset, and measure the accuracy on the same dataset. Second, we train the detector on combinations of datasets in order to judge the performance degradation due to the learning on the more types of cells. Because of the varying size of the cells in the datasets, and the varying brightness of the cells, we expect that such a trained detector will perform poorer than when trained and tested on individual datasets, sometimes mistakenly detecting small artefacts in the background as cells. Finally, we compute the average detection time per frame for each dataset.
	
	The aim of this research was to develop an automatic cell detection and tracking pipeline that would require as little manual work as possible. This implies that a balance between accuracy and amount of manual work had to be established. There is also an direct relationship between accuracy and computation time. In order to reduce the amount of manual work we aimed to configure the cell detection module such that it would perform well on all the tested datasets without any manual adjustments of parameters. The consequences of this decision are twofold:
	
	\begin{enumerate}
		\item The features computed on the candidate cell regions are the same for all datasets and have been presented in \cref{sec:detector_feature}. Although some datasets could be analysed faster or more accurately with a different subset of features, using the same features for all dataset eliminates the complicated feature selection process for the user and makes the system generalizable to a large number of different cell types.
		
		\item The parameters of the MSER detector should be adequately set to perform well on all datasets. This means that the MSER detector should be able to detect cells of varying size and contrast in the different datasets. The consequence of this limitation for datasets with large cells and some background noise is that a potentially much larger number of candidate regions will be detected than necessary. Since each candidate region has to be evaluated this results in an increased computation time.
	\end{enumerate}
	
	We were able to identify features that compute in an acceptable time for all these datasets (see \cref{sec:detector_feature}). However, it should be noted that in the case of testing the detector on very large datasets with thousands of frames, some adjustments of the parameters could result in a significant reduction in computation time and increased accuracy.
	
	\subsection{Performance metrics \statusfirstdraft}
	
	We measure the performance of the cell detector in terms of precision and recall. The metrics are defined in terms of:
	
	\begin{description}
		\item[True Positive instances] (TP) are candidate cell regions that are manually annotated as cells and the detector successfully classified as cells.
		\item[False Positive instances] (FP) are candidate cell regions that are not manually annotated as cells, but the detector incorrectly classified them as cells.
		\item[False Negative instances] (FN) are candidate cell regions that are manually annotated as cells, but the detector incorrectly classified.
	\end{description}
	
	We then define precision as:
	
	\[
		\text{PRE} = \frac{\text{TP}}{\text{TP}+\text{FP}}\text{,}
	\]
	
	\noindent and recall (also known as sensitivity) as:
	
	\[
		\text{REC} = \frac{\text{TP}}{\text{TP} + \text{FN}}\ \text{.}
	\]
	
	\subsection{Detection accuracy \statusfirstdraft}
			
		As mentioned previously, we performed two different experiments to measure the performance of the cell detection module. The first experiment consisted of training and testing of the algorithm on the same dataset. The training set was 70\% and the testing set 30\% of the entire dataset. The training and testing datasets were created from images in randomized order from the entire dataset. This allowed us to measure, for each dataset, the maximum precision and recall values we could expect from the algorithm. In the second experiment the training was performed on combined datasets. The goal of this experiment was to observe how well the algorithm is able to generalize, and still return acceptable results.
		
		\subsubsection{Training and testing on individual datasets}
		
		The training was performed on 70\% of the manually dot-annotated images from each dataset, and tested on the remaining 30\%. \Cref{tab:results_detector_individual} displays the computed precision and recall values, together with the total number of cells that were annotated in each dataset.
		
		\todo{Repeat the measurements with the best feature selection}
		
		\begin{table}[h]
			\centering
			\begin{tabular}{rrr}
				Dataset & Precision & Recall \\
				\hline
				      A &      25.0 &   22.5 \\
				      B &      90.1 &   89.1 \\
				      C &      76.5 &   84.2 \\
				      D &      86.1 &   85.3 \\
				      E &      93.4 &   78.6
			\end{tabular} 
			\caption{Precision and recall values for the cell detector trained on each dataset individually.}
			\label{tab:results_detector_individual}
		\end{table}
		
		The detector tested on datasets B to E achieved precision and recall values above 75. A manual comparison of the annotation confirms that the results are good, with few bad detections. Most of the differences in detection were caused by minor inconsistencies in annotations. The importance of consistent annotations was stressed in \cref{sec:data_manual_annotation}.
		
		Dataset A is an outlier with extremely low precision and recall values. This is likely caused by the specific characteristics of this image sequence, which we described in \cref{subsec:datasetA}. Briefly, between frame 17 and 18 there is an abrupt change of image clarity. The background from the threshold frame onwards have a texture that is very similar to that of the cells in the first 17 frames. This means that the negative candidate regions from frame 18 and onward clash with the positive candidate regions from the previous frames. The result is that the detector is unable to learn to discriminate cells from background.
		
		These results show us the detector can correctly detect most of the (annotated) cells. Further manual reviews of the annotations would likely improve the performance, but this is not done here as it should have been tested on a separate validation dataset. Additionally, it is unlikely that future users of this detection method will always have perfect annotations available. In the next experiment, we will measured the performance of the detector when trained on a composite dataset. This will tell us whether it would be possible to learn a single, general detector and use it to test on a new, possibly unforeseen datasets.

		\Cref{fig:results_detector_sequences_1,fig:results_detector_sequences_2,fig:results_detector_sequences_3,fig:results_detector_sequences_4,fig:results_detector_sequences_5} in appendix \ref{app:appendix_detectionresults} display a temporal view of the detected results in each datasets. The vertical axis represents the consecutive frame number of the image sequence. The figures show that ``cell tracks'' are discernible, even if the number of outliers is significant. The detectors used to detect the cells on the entire dataset were trained on all the annotated frames. \todo{Repeat the training+testing using all frames for training}

		\subsubsection{Training on combined datasets}
		
		In this second experiment we were interested in measuring how well the algorithm is able to generalize when it is trained on a larger, combined dataset. For this purpose, several of the datasets were grouped, and the detector was trained to recognize cells of all types. The detector was trained on a random 70\% of all annotated images in the combined dataset. It was then tested on a random 30\% of annotated frames from each individual dataset separately. This means that sometimes the same frames could be used for training and testing. However, we also run combinations of datasets where one dataset is left out of training at each time. Testing on that dataset should then reveal if the algorithm generalizes well.
		
		\Cref{tab:results_detector_combined} summarizes the precision and recall values for all tested combined datasets. The column denoted $\gamma$ contains testing performance as tested on 30\% of the combined dataset. The values shown in bold correspond to the performance of testing on the dataset that was left out from the combined training dataset. The row denoted ``Individual'' indicates the results when the datasets where trained and tested individually (these are the same results as in \cref{tab:results_detector_individual}). The values shown in red indicate a decrease in precision/recall by at least 1 point compared to the results when trained and tested on individual datasets. Similarly, blue colour indicates an increase in precision/recall compared to the results on the row denoted ``Individual''.
		
		\todo{Repeat the measurements with the best feature selection}
		
		\begin{table}[h]
			\centering
			\begin{tabular}{c|*{6}{c}|*{6}{c}}
				\multicolumn{1}{c}{Dataset} & \multicolumn{6}{|c|}{Precision}                                              &                          \multicolumn{6}{c}{Recall}                          \\
				                            & $\gamma$ &     A      &     B      &     C      &     D      &       E       & $\gamma$ &     A      &       B       &     C      &     D      &     E      \\
				        Individual          &          &    25.0    &    90.1    &    76.5    &    86.1    &     93.4      &          &    22.5    &     89.1      &    84.2    &    85.3    &    78.6    \\
			\hline
				           ABCDE            &   74.5   & \up{49.2}  & \dn{76.0}  & \up{85.2}  & \dn{69.2}  &   \up{94.5}   &   79.9   & \up{58.3}  &   \up{90.8}   & \dn{57.1}  & \up{98.9}  & \up{79.7}  \\
			\hline
				          ABCD\_            &   69.7   & \up{49.2}  & \dn{77.0}  & \up{85.8}  & \dn{69.9}  & \textbf{93.4} &   80.0   & \up{58.3}  &     90.3      & \dn{58.7}  & \up{98.4}  & \upd{80.7} \\
				          ABC\_E            &   72.8   & \up{45.9}  & \dn{75.0}  & \up{82.6}  & \dnd{41.1} &   \dn{87.2}   &   83.2   & \up{65.0}  &   \up{96.4}   & \dn{73.6}  & \upd{99.4} & \up{89.8}  \\
				          AB\_DE            &   75.5   & \up{47.5}  & \dn{75.8}  & \upd{85.7} & \dn{73.1}  &   \up{96.5}   &   73.1   & \up{50.8}  &     88.9      & \dnd{46.1} & \up{97.1}  & \dn{68.3}  \\
				          A\_CDE            &   75.9   & \up{41.3}  & \dnd{63.1} & \up{85.5}  & \dn{71.2}  &   \up{94.6}   &   70.4   & \up{58.3}  & \textbf{89.2} & \dn{57.0}  & \up{99.0}  & \up{80.2}  \\
				          \_BCDE            &   86.3   & \upd{49.2} & \dn{76.4}  & \up{85.9}  & \dn{71.8}  &   \up{94.4}   &   76.4   & \upd{58.3} &     89.5      & \dn{56.0}  & \up{97.4}  & \dn{75.7}  \\
			\hline
				          \_BCD\_           &   82.8   & \upd{52.5} & \dn{78.0}  & \up{87.5}  & \dn{72.7}  &  \upd{95.2}   &   74.5   & \upd{55.8} &     89.5      & \dn{54.7}  & \up{97.4}  & \dnd{74.0} \\
				          \_B\_DE           &   85.9   & \upd{42.5} & \dn{80.3}  & \upd{84.8} & \dn{72.1}  &   \up{97.7}   &   82.6   & \upd{45.8} &   \up{91.2}   & \dnd{44.8} & \up{97.4}  &  \dn{66.3}
			\end{tabular} 
			\caption{Precision and recall values for the cell detector trained on combined datasets. Values typed in bold indicate the testing datasets that were not included in the combined training dataset. Red/blue colour indicates a decrease/increase in performance compared to training and testing on each individual dataset by at least 1 precision/recall point.}
			\label{tab:results_detector_combined}
		\end{table}
		
		The values in the table show us some significant insights. First of all, the recall values increased for most datasets when detecting cells using a detector trained with almost any combination of data. Using more data thus improves the recall. This means that in general the algorithm will detect a larger number of true positive instances, but also some false negatives. However, given the high recall value we can expect the number of false negatives to remain relatively small. For the tracking module it is easy to discard short, isolated detections. The high recall values indicate that we will likely detect most of the real cell trajectories.
		
		Second, we notice that while most recall increase, precision tends to decrease in some datasets. This means that the number of false positives increased. As said beforehand, the cell tracking module can deal effectively with short, isolated false positives. However, if the number of false positives increases too much, the tracking module might detect cell trajectories where there are none (or correspond to background noise). However, given that in most examples the precision values did not fall below 70 we should still expect to achieve good tracking results.
		
		The results on Dataset C went against these observations. Its recall values decreased significantly whenever a combined dataset was used to train the detector. This dataset contains many motion artefacts, and high variance of intensities of the cells, some of which smoothly blend into the background. This makes it very difficult to consistently annotate. It is possible that the inconsistencies in annotation make it perform in such a way when combined with other datasets.
		
		Another interesting observation is that the precision and recall values for dataset A improved significantly compared to training the detector only on dataset A. We have previously presented some possible reasons for its poor performance when trained individually. The increased performance when using a cell detector trained on other datasets (even excluding dataset A itself) can be attributed to two things. First, the detector learns to detect a wider range of types of cells. Second, dataset A contains much fewer annotated cells than the other datasets (about five times less). It is possible that a detector trained on only dataset A overfits the annotations, and cannot generalize to detect cells in other frames. 
		
		The performance on dataset E remained almost constant, with very few exceptions (e.g. smaller recall value when the detector was trained on combined dataset containing datasets A, B, D and E).

		In this section we aimed to understand whether a detector trained on a single, combined dataset could be used to detect cells in new, previously unseen images. The results have shown that a general detector will not perform well on \textit{all} new unseen datasets. For example, we have measured a significant drop in precision values when testing on unseen datasets B and D, and a significant drop in recall values when testing on dataset C. However, in some cases training on a combined detector either improved the performance or reduced it insignificantly (such as in datasets A and E). What this tells us is that when presented with a new dataset it is worth trying to use a pre-trained detector and review its results. In some cases this could give as acceptable results, and manual annotation of the new dataset might be unnecessary.
		
		It is worth keeping in mind that all five datasets analysed in this thesis show distinct characteristics (both in cell type and image clarity). In practice, datasets will often be similar and it might be sufficient to annotated a single dataset to train a detector that can be used on other similar samples.
	
	\subsection{Computations time \statusfirstdraft}
	
		In order to positionally track cells in image sequence the computation time of extracting the cell position from each frame is just as important as the accurate identification of cells. \Cref{tab:results_detector_speed} displays the average detection times per frame for each dataset. We also measured the average detection time per annotated cell in each dataset and the average ratio between the number of all candidate regions that were detected with the MSER detector and the number of annotated cells in each frame.
		
		The detection was performed on a PC with an Intel(R) Core(TM) i7-2600 CPU with a clock frequency of 3.40GHz and 8GB RAM. The MATLAB version used to measure the detection speed was 8.1.0.604 (R2013a) running in Ubuntu Linux 13.04 x64. Althought the detector can be easily configured to use several workers to process the image sequences in parallel, the measurements were performed using a single worker, i.e. all images was processed sequentially.
		
		\begin{table}[h]
			\centering
			\begin{tabular}{rrrr}
				Dataset & Time per frame  $\left[ s \right]$ & Time per annotated cell  $\left[ s \right]$ & Candidate-Annotation ratio \\
			\hline
				      A &                             0.9224 &                                      0.5854 &                       26:1 \\
				      B &                             1.5899 &                                      0.2015 &                       16:1 \\
				      C &                             1.1214 &                                      0.0586 &                        7:1 \\
				      D &                             1.5215 &                                      0.1454 &                       18:1 \\
				      E &                             0.6016 &                                      0.0853 &                        9:1
			\end{tabular} 
			\caption{Average computation times per frame and annotated cell.}
			\label{tab:results_detector_speed}
		\end{table}
		
		The measurements show that the detection speed is dependent on the number of candidate cell regions extracted and their dimensions. Datasets A and B contain larger cells than the remaining three. Consequently, the time per candidate region considered is larger (0.0225 and 0.0126 seconds for regions in datasets A and B, correspondingly) than that of regions in  datasets C, D and E (0.0084, 0.0081 and 0.0095 seconds, correspondingly).
		
		This means that if speed becomes a bottleneck in the analysis of large datasets, the images could be scaled down before they are analysed by the cell detector.
		
		The results show that we were able to optimize the cell detector to a point where it's speed is no longer an issue for many use cases. As a reminder, the original paper Arteta \emph{et. al.} \cite{arteta12} reports detection speeds of 30 seconds per 400-by-400 pixels image on an i7 CPU.
		
		\todo{It might be useful to show the comparison with the old detector if I have the time}
		
\section{Cell tracker \statusnew}
	\label{sec:results_tracker}
	\todo[inline]{Define the different measures of accuracy}
	\subsection{Performance metrics \statusnew}
	
	TODO: test detector on full dataset, but train it on ALL (notjust70\%) of the annotation
	TODO: train tracker on 70\% of the trajectories, test on rest
	
	great Metrics: Research Article, Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics
	
	- Explain how the testing data was generation. from annotation to mapped deteciton to genreated tracklets

	\subsection{Tracking accuracy \statusnew}
		- trained on single dataset
		
		- for each dataset explain how the parameters were setup
					
		- trained on combined dataset
	\subsection{Computation time \statusnew}
	\todo[inline]{Meause the speed of generating tracks, as a measure of per 1, 100, 1000 frames, depending on the number of tracks}
\section{Limitations and areas of improvement \statusnew}
	\label{sec:results_limitations}
	Answer: what, why, how to improve in future
			
	- display examples where the tracker did not perform well, and anlyse why. Suggest possible improvement.
	- detection training: only first few frames of datasets, not random -- expect to detect later frames worse
	- testing on only long datasets: no data on short datasets. diffucult to train (what to link?), difficult to annotate
	- speed of detector. Reduce number of hypothesis		

\section{Summary \statusnew}
	\label{sec:results_summary}
	Brief review of accuracy... whether it is comparable to other methods in literuature review
	Whether is could be improved in the future... how much